{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "OOktbwn1l9R0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"myAPP\").getOrCreate()"
      ],
      "metadata": {
        "id": "u_x6mH5mmDn8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw Data"
      ],
      "metadata": {
        "id": "v97NfsnS0M-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_df = pd.read_csv(\"orders_large_bad.csv\", low_memory=False)\n",
        "json_df = pd.read_json(\"orders_large_bad.json\", lines=True)\n",
        "\n",
        "df = pd.concat([csv_df, json_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "INKRQ4l6v4ji"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "df[\"order_id\"] = pd.to_numeric(df[\"order_id\"], errors=\"coerce\")\n",
        "df[\"customer_id\"] = pd.to_numeric(df[\"customer_id\"], errors=\"coerce\")\n",
        "df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
        "\n",
        "df[\"order_date\"] = pd.to_datetime(df[\"order_date\"], errors=\"coerce\")\n",
        "\n",
        "df = df.dropna(subset=[\"order_id\", \"customer_id\", \"order_date\"])\n",
        "\n",
        "df.to_csv(\"orders_clean.csv\", index=False)"
      ],
      "metadata": {
        "id": "38yzNqRkwCLJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OrdersDataPipeline\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"order_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"category\", StringType(), True),\n",
        "    StructField(\"product\", StringType(), True),\n",
        "    StructField(\"amount\", DoubleType(), True),\n",
        "    StructField(\"order_date\", DateType(), True),\n",
        "    StructField(\"status\", StringType(), True)\n",
        "])\n",
        "\n",
        "orders_df = spark.read.csv(\"orders_clean.csv\", header=True, schema=schema)\n",
        "\n",
        "orders_df.printSchema()\n",
        "orders_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te8fCBVdxJNu",
        "outputId": "f0245663-1196-4fb0-b399-a6e6df90ec89"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n",
            "+--------+-----------+----+--------+-------+------+----------+------+\n",
            "|order_id|customer_id|city|category|product|amount|order_date|status|\n",
            "+--------+-----------+----+--------+-------+------+----------+------+\n",
            "+--------+-----------+----+--------+-------+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df = orders_df.dropDuplicates([\"order_id\"])\n",
        "\n",
        "from pyspark.sql.functions import trim, lower\n",
        "orders_df = orders_df.withColumn(\"city\", trim(lower(orders_df[\"city\"]))) \\\n",
        "                     .withColumn(\"category\", trim(lower(orders_df[\"category\"]))) \\\n",
        "                     .withColumn(\"status\", trim(lower(orders_df[\"status\"])))\n",
        "\n",
        "\n",
        "orders_df.write.mode(\"overwrite\").parquet(\"orders_parquet\")"
      ],
      "metadata": {
        "id": "Q4_tOjBXxQ5H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df = orders_df.withColumn(\n",
        "    \"amount_int\",\n",
        "    when(col(\"amount\").rlike(\"^[0-9]+$\"), col(\"amount\").cast(IntegerType())).otherwise(None)\n",
        ")\n"
      ],
      "metadata": {
        "id": "R1S_zI3fI92V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum as _sum, avg, max, min, col, when\n",
        "\n",
        "# Revenue summary by category\n",
        "revenue_by_category = orders_df.groupBy(\"category\").agg(\n",
        "    _sum(col(\"amount_int\")).alias(\"total_revenue\"),\n",
        "    avg(col(\"amount_int\")).alias(\"avg_revenue\"),\n",
        "    max(col(\"amount_int\")).alias(\"max_revenue\"),\n",
        "    min(col(\"amount_int\")).alias(\"min_revenue\")\n",
        ")\n",
        "\n",
        "revenue_by_category.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYhOMcqgxYa6",
        "outputId": "7ed34f70-061c-43d7-a690-e05f1a3e3cac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------+-----------+-----------+-----------+\n",
            "|category|total_revenue|avg_revenue|max_revenue|min_revenue|\n",
            "+--------+-------------+-----------+-----------+-----------+\n",
            "+--------+-------------+-----------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "revenue_by_category.explain(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEl0wrYLJZFS",
        "outputId": "c87f77da-d9d5-4653-d4ff-1a6176f7e970"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['category], ['category, 'sum('amount_int) AS total_revenue#157, 'avg('amount_int) AS avg_revenue#158, 'max('amount_int) AS max_revenue#159, 'min('amount_int) AS min_revenue#160]\n",
            "+- Project [order_id#0, customer_id#1, city#42, category#43, product#4, amount#5, order_date#6, status#44, CASE WHEN RLIKE(cast(amount#5 as string), ^[0-9]+$) THEN cast(amount#5 as int) ELSE cast(null as int) END AS amount_int#156]\n",
            "   +- Project [order_id#0, customer_id#1, city#42, category#43, product#4, amount#5, order_date#6, trim(lower(status#7), None) AS status#44]\n",
            "      +- Project [order_id#0, customer_id#1, city#42, trim(lower(category#3), None) AS category#43, product#4, amount#5, order_date#6, status#7]\n",
            "         +- Project [order_id#0, customer_id#1, trim(lower(city#2), None) AS city#42, category#3, product#4, amount#5, order_date#6, status#7]\n",
            "            +- Deduplicate [order_id#0]\n",
            "               +- Relation [order_id#0,customer_id#1,city#2,category#3,product#4,amount#5,order_date#6,status#7] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "category: string, total_revenue: bigint, avg_revenue: double, max_revenue: int, min_revenue: int\n",
            "Aggregate [category#43], [category#43, sum(amount_int#156) AS total_revenue#157L, avg(amount_int#156) AS avg_revenue#158, max(amount_int#156) AS max_revenue#159, min(amount_int#156) AS min_revenue#160]\n",
            "+- Project [order_id#0, customer_id#1, city#42, category#43, product#4, amount#5, order_date#6, status#44, CASE WHEN RLIKE(cast(amount#5 as string), ^[0-9]+$) THEN cast(amount#5 as int) ELSE cast(null as int) END AS amount_int#156]\n",
            "   +- Project [order_id#0, customer_id#1, city#42, category#43, product#4, amount#5, order_date#6, trim(lower(status#7), None) AS status#44]\n",
            "      +- Project [order_id#0, customer_id#1, city#42, trim(lower(category#3), None) AS category#43, product#4, amount#5, order_date#6, status#7]\n",
            "         +- Project [order_id#0, customer_id#1, trim(lower(city#2), None) AS city#42, category#3, product#4, amount#5, order_date#6, status#7]\n",
            "            +- Deduplicate [order_id#0]\n",
            "               +- Relation [order_id#0,customer_id#1,city#2,category#3,product#4,amount#5,order_date#6,status#7] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [category#43], [category#43, sum(amount_int#156) AS total_revenue#157L, avg(amount_int#156) AS avg_revenue#158, max(amount_int#156) AS max_revenue#159, min(amount_int#156) AS min_revenue#160]\n",
            "+- Project [category#43, CASE WHEN RLIKE(cast(amount#265 as string), ^[0-9]+$) THEN cast(amount#265 as int) END AS amount_int#156]\n",
            "   +- Aggregate [order_id#0], [trim(lower(first(category#3, false)), None) AS category#43, first(amount#5, false) AS amount#265]\n",
            "      +- Project [order_id#0, category#3, amount#5]\n",
            "         +- Relation [order_id#0,customer_id#1,city#2,category#3,product#4,amount#5,order_date#6,status#7] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[category#43], functions=[sum(amount_int#156), avg(amount_int#156), max(amount_int#156), min(amount_int#156)], output=[category#43, total_revenue#157L, avg_revenue#158, max_revenue#159, min_revenue#160])\n",
            "   +- Exchange hashpartitioning(category#43, 200), ENSURE_REQUIREMENTS, [plan_id=246]\n",
            "      +- HashAggregate(keys=[category#43], functions=[partial_sum(amount_int#156), partial_avg(amount_int#156), partial_max(amount_int#156), partial_min(amount_int#156)], output=[category#43, sum#198L, sum#199, count#200L, max#201, min#202])\n",
            "         +- Project [category#43, CASE WHEN RLIKE(cast(amount#265 as string), ^[0-9]+$) THEN cast(amount#265 as int) END AS amount_int#156]\n",
            "            +- SortAggregate(key=[order_id#0], functions=[first(category#3, false), first(amount#5, false)], output=[category#43, amount#265])\n",
            "               +- Sort [order_id#0 ASC NULLS FIRST], false, 0\n",
            "                  +- Exchange hashpartitioning(order_id#0, 200), ENSURE_REQUIREMENTS, [plan_id=240]\n",
            "                     +- SortAggregate(key=[order_id#0], functions=[partial_first(category#3, false), partial_first(amount#5, false)], output=[order_id#0, first#274, valueSet#275, first#276, valueSet#277])\n",
            "                        +- Sort [order_id#0 ASC NULLS FIRST], false, 0\n",
            "                           +- FileScan csv [order_id#0,category#3,amount#5] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders_clean.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:int,category:string,amount:double>\n",
            "\n"
          ]
        }
      ]
    }
  ]
}