{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn9fPHGanXpCpwUzLpaT/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Silvio-0-1/Python-Training/blob/main/16-12-2025/pyspark/knowledge_on_dag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APACHE SPARK EXERCISES"
      ],
      "metadata": {
        "id": "VWQw_ATXfikQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topics:**\n",
        "\n",
        "1. DAG (Directed Acyclic Graph)\n",
        "2. Broadcast Joins\n",
        "3. Transformations\n",
        "4. Actions\n",
        "5. Partitions"
      ],
      "metadata": {
        "id": "Lm6Cq_L9kFyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('partitioning').getOrCreate()"
      ],
      "metadata": {
        "id": "1Dy-HPqcfZXr"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, broadcast"
      ],
      "metadata": {
        "id": "UuxUGpx6fbSJ"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DOMAIN:** Ride-Hailing Platform Analytics (Uber/Ola)"
      ],
      "metadata": {
        "id": "Cd2TJVi4e-wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET 1 — RIDES (Large Fact Table)\n",
        "\n",
        "rides_data = [\n",
        "    (\"R001\",\"U001\",\"Hyderabad\",12.5,240,\"Completed\"),\n",
        "    (\"R002\",\"U002\",\"Delhi\",8.2,180,\"Completed\"),\n",
        "    (\"R003\",\"U003\",\"Mumbai\",15.0,300,\"Cancelled\"),\n",
        "    (\"R004\",\"U004\",\"Bangalore\",5.5,120,\"Completed\"),\n",
        "    (\"R005\",\"U005\",\"Hyderabad\",20.0,360,\"Completed\"),\n",
        "    (\"R006\",\"U006\",\"Delhi\",25.0,420,\"Completed\"),\n",
        "    (\"R007\",\"U007\",\"Mumbai\",7.5,150,\"Completed\"),\n",
        "    (\"R008\",\"U008\",\"Bangalore\",18.0,330,\"Completed\"),\n",
        "    (\"R009\",\"U009\",\"Delhi\",6.0,140,\"Cancelled\"),\n",
        "    (\"R010\",\"U010\",\"Hyderabad\",10.0,200,\"Completed\")\n",
        "]\n",
        "\n",
        "rides_cols = [\n",
        "    \"ride_id\",\n",
        "    \"user_id\",\n",
        "    \"city\",\n",
        "    \"distance_km\",\n",
        "    \"duration_seconds\",\n",
        "    \"status\"\n",
        "]\n",
        "\n",
        "rides_df = spark.createDataFrame(rides_data, rides_cols)"
      ],
      "metadata": {
        "id": "QS0_FuDMf8om"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET 2 — CITY SURGE MULTIPLIERS (Small Lookup)\n",
        "\n",
        "surge_data = [\n",
        "    (\"Hyderabad\", 1.2),\n",
        "    (\"Delhi\", 1.5),\n",
        "    (\"Mumbai\", 1.8),\n",
        "    (\"Bangalore\", 1.3)\n",
        "]\n",
        "\n",
        "surge_cols = [\"city\", \"surge_multiplier\"]\n",
        "\n",
        "surge_df = spark.createDataFrame(surge_data, surge_cols)"
      ],
      "metadata": {
        "id": "X59aq-eIgVa2"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SET 1: TRANSFORMATIONS vs ACTIONS**\n",
        "\n"
      ],
      "metadata": {
        "id": "7L6oielcg0gW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1.1:** Create a transformation pipeline that:\n",
        "\n",
        "1. Filters only Completed rides\n",
        "2. Selects ride_id , city , distance_km\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Do not trigger any action.\n",
        "2. Explain whether Spark executed anything."
      ],
      "metadata": {
        "id": "44G8L7Npg50R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filters and selects without triggering action\n",
        "filtered_rides_df = rides_df.select(col(\"ride_id\"), col(\"city\"), col(\"distance_km\")).filter((col(\"status\") == \"Completed\"))"
      ],
      "metadata": {
        "id": "w1UZOe4khvk5"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2:** Explain whether Spark executed anything.\n",
        "\n",
        "> Because Spark uses **lazy evaluation**, it has not yet executed any computations. The transformations are only recorded as a logical plan and will only be executed when an action is called on the **filtered_rides_df** DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "cES3LpsTpWOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1.2:** Trigger a single action on the pipeline.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Identify which line caused execution.\n",
        "2. Explain why previous lines did not execute."
      ],
      "metadata": {
        "id": "Jdk9uHP4fF6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trigger action on the pipeline\n",
        "filtered_rides_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XK1WXA5i4lM",
        "outputId": "fbfc2e47-1eac-4796-b705-8ff3c3f35337"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-----------+\n",
            "|ride_id|     city|distance_km|\n",
            "+-------+---------+-----------+\n",
            "|   R001|Hyderabad|       12.5|\n",
            "|   R002|    Delhi|        8.2|\n",
            "|   R004|Bangalore|        5.5|\n",
            "|   R005|Hyderabad|       20.0|\n",
            "|   R006|    Delhi|       25.0|\n",
            "|   R007|   Mumbai|        7.5|\n",
            "|   R008|Bangalore|       18.0|\n",
            "|   R010|Hyderabad|       10.0|\n",
            "+-------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1:** Identify which line caused execution.\n",
        "> The line **filtered_rides_df.show()** is the action that triggered the execution of the transformation pipeline."
      ],
      "metadata": {
        "id": "jOmNK4vfqHu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2:** Explain why previous lines did not execute.\n",
        "> Previous lines did not execute immediately because Spark employs **lazy evaluation**."
      ],
      "metadata": {
        "id": "Mdge60PpvMui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SET 2: DAG & LINEAGE**"
      ],
      "metadata": {
        "id": "Q0uEp7MdfK1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2.1:** Create a transformation chain with:\n",
        "\n",
        "1. Multiple filters\n",
        "2. A column selection\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Run explain(True)\n",
        "\n",
        "2. Identify:\n",
        "\n",
        "* Logical plan\n",
        "* Optimized logical plan\n",
        "* Physical plan"
      ],
      "metadata": {
        "id": "ih6Tgun_jBZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# multiple filters and column selection\n",
        "transformed_rides_df = rides_df.filter(col(\"status\") == \"Completed\").filter(col(\"distance_km\") > 10).select(\"ride_id\", \"city\", \"distance_km\", \"status\")\n",
        "\n",
        "# run explain\n",
        "transformed_rides_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9pI_TaZjpJx",
        "outputId": "f497ff4d-1d08-4ebe-be0c-b52b0f2a1fc7"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project ['ride_id, 'city, 'distance_km, 'status]\n",
            "+- Filter (distance_km#365 > cast(10 as double))\n",
            "   +- Filter (status#367 = Completed)\n",
            "      +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, city: string, distance_km: double, status: string\n",
            "Project [ride_id#362, city#364, distance_km#365, status#367]\n",
            "+- Filter (distance_km#365 > cast(10 as double))\n",
            "   +- Filter (status#367 = Completed)\n",
            "      +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [ride_id#362, city#364, distance_km#365, status#367]\n",
            "+- Filter ((isnotnull(status#367) AND isnotnull(distance_km#365)) AND ((status#367 = Completed) AND (distance_km#365 > 10.0)))\n",
            "   +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [ride_id#362, city#364, distance_km#365, status#367]\n",
            "+- *(1) Filter ((isnotnull(status#367) AND isnotnull(distance_km#365)) AND ((status#367 = Completed) AND (distance_km#365 > 10.0)))\n",
            "   +- *(1) Scan ExistingRDD[ride_id#362,user_id#363,city#364,distance_km#365,duration_seconds#366L,status#367]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2.2:** Reorder transformations (filter after join vs before join).\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Compare DAGs.\n",
        "2. Identify which plan is more efficient and why."
      ],
      "metadata": {
        "id": "UDILIeFujjnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter before join\n",
        "filtered_rides_df_join = rides_df.filter(col(\"distance_km\") > 10)\n",
        "filtered_then_joined_df = filtered_rides_df_join.join(surge_df, on=\"city\", how=\"inner\")\n",
        "\n",
        "filtered_then_joined_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWoQ-084q_-G",
        "outputId": "f4aa7520-5e73-46be-eadc-d6f33fde14dd"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Filter (distance_km#365 > cast(10 as double))\n",
            ":  +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "+- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "+- Join Inner, (city#364 = city#368)\n",
            "   :- Filter (distance_km#365 > cast(10 as double))\n",
            "   :  +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "   +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "+- Join Inner, (city#364 = city#368)\n",
            "   :- Filter ((isnotnull(distance_km#365) AND (distance_km#365 > 10.0)) AND isnotnull(city#364))\n",
            "   :  +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "   +- Filter isnotnull(city#368)\n",
            "      +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "   +- SortMergeJoin [city#364], [city#368], Inner\n",
            "      :- Sort [city#364 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#364, 200), ENSURE_REQUIREMENTS, [plan_id=3484]\n",
            "      :     +- Filter ((isnotnull(distance_km#365) AND (distance_km#365 > 10.0)) AND isnotnull(city#364))\n",
            "      :        +- Scan ExistingRDD[ride_id#362,user_id#363,city#364,distance_km#365,duration_seconds#366L,status#367]\n",
            "      +- Sort [city#368 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#368, 200), ENSURE_REQUIREMENTS, [plan_id=3485]\n",
            "            +- Filter isnotnull(city#368)\n",
            "               +- Scan ExistingRDD[city#368,surge_multiplier#369]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter after join\n",
        "joined_then_filtered_df = rides_df.join(surge_df, on=\"city\", how=\"inner\").filter(col(\"distance_km\") > 10)\n",
        "\n",
        "joined_then_filtered_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWWBiVpVsp_o",
        "outputId": "65b803f0-c977-4353-bfc4-724a04e64e42"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Filter '`>`('distance_km, 10)\n",
            "+- Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "   +- Join Inner, (city#364 = city#368)\n",
            "      :- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "      +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Filter (distance_km#365 > cast(10 as double))\n",
            "+- Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "   +- Join Inner, (city#364 = city#368)\n",
            "      :- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "      +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "+- Join Inner, (city#364 = city#368)\n",
            "   :- Filter ((isnotnull(distance_km#365) AND (distance_km#365 > 10.0)) AND isnotnull(city#364))\n",
            "   :  +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "   +- Filter isnotnull(city#368)\n",
            "      +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "   +- SortMergeJoin [city#364], [city#368], Inner\n",
            "      :- Sort [city#364 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#364, 200), ENSURE_REQUIREMENTS, [plan_id=3515]\n",
            "      :     +- Filter ((isnotnull(distance_km#365) AND (distance_km#365 > 10.0)) AND isnotnull(city#364))\n",
            "      :        +- Scan ExistingRDD[ride_id#362,user_id#363,city#364,distance_km#365,duration_seconds#366L,status#367]\n",
            "      +- Sort [city#368 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#368, 200), ENSURE_REQUIREMENTS, [plan_id=3516]\n",
            "            +- Filter isnotnull(city#368)\n",
            "               +- Scan ExistingRDD[city#368,surge_multiplier#369]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1: Compare DAGs.**\n",
        "\n",
        "> In both scenarios, the **Optimized Logical Plan** and the **Physical Plan** look identical. This is a great demonstration of Spark's **Catalyst Optimizer**. Even when we explicitly write the filter *after* the join in the code, the optimizer is smart enough to push the filter down (known as **Predicate Pushdown**) to **rides_df** *before* the join operation.\n",
        "\n",
        "> This means that in both cases, the filter **distance_km > 10.0** is applied to the **rides_df** *before* it participates in the **SortMergeJoin** and subsequent **Exchange** (shuffle) operations.\n",
        "\n"
      ],
      "metadata": {
        "id": "SeJ7sXeEthx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: Which plan is more efficient?**\n",
        "\n",
        "> Because Spark's Catalyst Optimizer applies **Predicate Pushdown**, both explicit coding strategies (filter before join vs. filter after join) result in the **same efficient physical plan**."
      ],
      "metadata": {
        "id": "w5JGRvvAuKmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SET 3: PARTITIONS & SHUFFLE**"
      ],
      "metadata": {
        "id": "d3rJ4N7ZfN9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3.1:** Check the number of partitions of rides_df .\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Repartition into 4 partitions\n",
        "2. Coalesce into 1 partition\n",
        "3. Observe number of output files when writing to Parquet"
      ],
      "metadata": {
        "id": "yQA-SB-zkctm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of patitions\n",
        "rides_df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPKgBXzikqo3",
        "outputId": "c1eae679-bd61-4f46-b7cf-960aa71b49f4"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repartition into 4 partitions\n",
        "rides_df_repart = rides_df.repartition(4)\n",
        "rides_df_repart.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp312WH7k6tE",
        "outputId": "5c8abf98-e284-407a-d9cd-06a812e1e04a"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coalesce into 1 partition\n",
        "rides_df_coalesce = rides_df_repart.coalesce(1)\n",
        "rides_df_coalesce.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz5HSJ7klSo7",
        "outputId": "029e914d-9551-456d-c390-e6da2a5b7454"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rides_df_coalesce.write.mode(\"overwrite\").parquet(\"rides_df_coalesce.parquet\")"
      ],
      "metadata": {
        "id": "zswHJX0tleNp"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3.2:** Repartition rides by city.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Run explain(True).\n",
        "2. Identify whether a shuffle is introduced."
      ],
      "metadata": {
        "id": "1I6FYOEhkeEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# repartition rides\n",
        "rides_df_repartition = rides_df.repartition(col(\"city\"))\n",
        "rides_df_repartition.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4_UmccOmYR3",
        "outputId": "fdbfa490-741f-4001-b0c4-95e06b550e6b"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run explain\n",
        "rides_df_repartition.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aq1a4NPmT4_",
        "outputId": "5a980e50-8542-457f-b808-fce667f6deae"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'RepartitionByExpression ['city]\n",
            "+- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, user_id: string, city: string, distance_km: double, duration_seconds: bigint, status: string\n",
            "RepartitionByExpression [city#364]\n",
            "+- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "RepartitionByExpression [city#364]\n",
            "+- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- AQEShuffleRead coalesced\n",
            "      +- ShuffleQueryStage 0\n",
            "         +- Exchange hashpartitioning(city#364, 200), REPARTITION_BY_COL, [plan_id=3625]\n",
            "            +- *(1) Scan ExistingRDD[ride_id#362,user_id#363,city#364,distance_km#365,duration_seconds#366L,status#367]\n",
            "+- == Initial Plan ==\n",
            "   Exchange hashpartitioning(city#364, 200), REPARTITION_BY_COL, [plan_id=3618]\n",
            "   +- Scan ExistingRDD[ride_id#362,user_id#363,city#364,distance_km#365,duration_seconds#366L,status#367]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2:** Identify whether a shuffle is introduced.\n",
        "\n",
        "> Yes, a shuffle is introduced. In the **Physical Plan** we can see Exchange hashpartitioning (city#140, 200), REPARTITION_BY_COL.\n",
        "\n"
      ],
      "metadata": {
        "id": "yq_Q7OEZwVvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SET 4: JOIN WITHOUT BROADCAST (BAD DAG)**"
      ],
      "metadata": {
        "id": "MwLdEkf3fQKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4.1:** Join *rides_df* with *surge_df* on *city* without using broadcast.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Run explain(True)\n",
        "\n",
        "2. Identify:\n",
        "\n",
        "* Join type\n",
        "* Exchange operators\n",
        "* Sort operations\n",
        "* Stage boundaries"
      ],
      "metadata": {
        "id": "JVKv9Vrtmta2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join without broadcast\n",
        "joined_df = rides_df.join(surge_df, on = \"city\", how = \"inner\")"
      ],
      "metadata": {
        "id": "Uk-TEvN4nHjc"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run explain\n",
        "joined_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSh7Oab_ng9w",
        "outputId": "dcdcad87-b7ea-471e-cd97-487561ffd49f"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "+- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "+- Join Inner, (city#364 = city#368)\n",
            "   :- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "   +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "+- Join Inner, (city#364 = city#368)\n",
            "   :- Filter isnotnull(city#364)\n",
            "   :  +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "   +- Filter isnotnull(city#368)\n",
            "      +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "   +- SortMergeJoin [city#364], [city#368], Inner\n",
            "      :- Sort [city#364 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#364, 200), ENSURE_REQUIREMENTS, [plan_id=3658]\n",
            "      :     +- Filter isnotnull(city#364)\n",
            "      :        +- Scan ExistingRDD[ride_id#362,user_id#363,city#364,distance_km#365,duration_seconds#366L,status#367]\n",
            "      +- Sort [city#368 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#368, 200), ENSURE_REQUIREMENTS, [plan_id=3659]\n",
            "            +- Filter isnotnull(city#368)\n",
            "               +- Scan ExistingRDD[city#368,surge_multiplier#369]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4.2:** Apply a filter *(distance_km > 10)* before the join.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Observe whether shuffle is removed.\n",
        "2. Explain why or why not?"
      ],
      "metadata": {
        "id": "dHaj828rmvAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply a filter before the join\n",
        "filtered_rides_df_2 = rides_df.filter(col(\"distance_km\") > 10)\n",
        "joined_filtered_df_2 = filtered_rides_df_2.join(surge_df, on=\"city\", how=\"inner\")\n",
        "\n",
        "joined_filtered_df_2.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x336LLdZn2_B",
        "outputId": "3d76d475-2177-43f9-d4ed-3520f8ab6a51"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Filter (distance_km#365 > cast(10 as double))\n",
            ":  +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "+- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "+- Join Inner, (city#364 = city#368)\n",
            "   :- Filter (distance_km#365 > cast(10 as double))\n",
            "   :  +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "   +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "+- Join Inner, (city#364 = city#368)\n",
            "   :- Filter ((isnotnull(distance_km#365) AND (distance_km#365 > 10.0)) AND isnotnull(city#364))\n",
            "   :  +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "   +- Filter isnotnull(city#368)\n",
            "      +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "   +- SortMergeJoin [city#364], [city#368], Inner\n",
            "      :- Sort [city#364 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#364, 200), ENSURE_REQUIREMENTS, [plan_id=3689]\n",
            "      :     +- Filter ((isnotnull(distance_km#365) AND (distance_km#365 > 10.0)) AND isnotnull(city#364))\n",
            "      :        +- Scan ExistingRDD[ride_id#362,user_id#363,city#364,distance_km#365,duration_seconds#366L,status#367]\n",
            "      +- Sort [city#368 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#368, 200), ENSURE_REQUIREMENTS, [plan_id=3690]\n",
            "            +- Filter isnotnull(city#368)\n",
            "               +- Scan ExistingRDD[city#368,surge_multiplier#369]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tasks:** Observe whether shuffle is removed. Why or why not?\n",
        "\n",
        "> No, the shuffle was not removed.\n",
        "\n",
        "> This is because the default join strategy is SortMergeJoin, which requires both DataFrames to be co-partitioned (and sorted) by the join key, and this co-partitioning usually involves a shuffle."
      ],
      "metadata": {
        "id": "XjXCyBLO2L1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SET 5: BROADCAST JOIN (GOOD DAG)**"
      ],
      "metadata": {
        "id": "K7MNnIaeoOhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 5.1:** Apply a broadcast hint to *surge_df* .\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Run explain(True)\n",
        "\n",
        "2. Identify:\n",
        "\n",
        "* Join type\n",
        "* BroadcastExchange\n",
        "* Disappearance of shuffles"
      ],
      "metadata": {
        "id": "KovBBCQMfR-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join with broadcast\n",
        "broadcast_join_df = rides_df.join(broadcast(surge_df), on = \"city\", how = \"inner\")\n",
        "\n",
        "# run explain\n",
        "broadcast_join_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RPg-GQBXxR4X",
        "outputId": "b8f10ba8-3dfe-40a9-90da-29f49aecbf18"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "+- Join Inner, (city#364 = city#368)\n",
            "   :- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "+- Join Inner, (city#364 = city#368), rightHint=(strategy=broadcast)\n",
            "   :- Filter isnotnull(city#364)\n",
            "   :  +- LogicalRDD [ride_id#362, user_id#363, city#364, distance_km#365, duration_seconds#366L, status#367], false\n",
            "   +- Filter isnotnull(city#368)\n",
            "      +- LogicalRDD [city#368, surge_multiplier#369], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#364, ride_id#362, user_id#363, distance_km#365, duration_seconds#366L, status#367, surge_multiplier#369]\n",
            "   +- BroadcastHashJoin [city#364], [city#368], Inner, BuildRight, false\n",
            "      :- Filter isnotnull(city#364)\n",
            "      :  +- Scan ExistingRDD[ride_id#362,user_id#363,city#364,distance_km#365,duration_seconds#366L,status#367]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=3719]\n",
            "         +- Filter isnotnull(city#368)\n",
            "            +- Scan ExistingRDD[city#368,surge_multiplier#369]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 5.2:** Compare physical plans from:\n",
        "\n",
        "* Exercise 4.1\n",
        "* Exercise 5.1\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. List operators that disappeared.\n",
        "2. Explain performance impact."
      ],
      "metadata": {
        "id": "VHCMYfpRoWTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1:** List operators that disappeared.\n",
        "\n",
        "> In essence, the entire Sort operation and the Exchange hashpartitioning (shuffle) operation are removed. These are replaced by a single BroadcastExchange for the smaller surge_df in the BroadcastHashJoin plan."
      ],
      "metadata": {
        "id": "I-Ntf5RN3tvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: Performance Impact.**\n",
        "\n",
        "> The absence of Sort and Exchange hashpartitioning operators means that the costly operations of sorting large datasets and shuffling data across the network are avoided.\n",
        "\n",
        "> This allows for a much faster join operation compared to SortMergeJoin, which always requires sorting and potentially shuffling both datasets."
      ],
      "metadata": {
        "id": "wB5tNMG44U44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SET 6: DAG INTERPRETATION**"
      ],
      "metadata": {
        "id": "S9yYMSHjyFi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 6.1:** From the physical plan:\n",
        "\n",
        "1. Identify all expensive operators.\n",
        "2. Classify them as CPU, memory, or network heavy."
      ],
      "metadata": {
        "id": "t5FYSac8yMtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Spark's physical plans, the most expensive operators and their classifications are:\n",
        "\n",
        "1.  **Exchange:** This operator is introduced during shuffles. It is primarily:\n",
        "      *   **Network-heavy:** Due to data transfer across the cluster between executors.\n",
        "      *   **CPU-heavy:** For serialization, deserialization, and hashing of data before and after transfer.\n",
        "\n",
        "2.  **Sort:** This operator sorts data within partitions. It is:\n",
        "      *   **Memory-heavy:** To hold data being sorted in memory.\n",
        "      *   **CPU-heavy:** For comparison operations during the sorting process.\n",
        "\n",
        "\n",
        "3.  **BroadcastExchange**: This operator is specific to where a smaller DataFrame is broadcast to all worker nodes. It is:\n",
        "      *   **Memory-heavy:** The entire broadcasted table must fit into the memory of each executor.\n",
        "      *   **Network-heavy:** To transfer the small table to all executors.\n",
        "\n",
        "4.  **SortMergeJoin / ShuffleHashJoin**: These are composite operations that inherently include `Exchange` and/or `Sort` operators, inheriting their costs."
      ],
      "metadata": {
        "id": "7yUKHYYO5Rmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 6.2:** Explain why Spark defaults to SortMergeJoin."
      ],
      "metadata": {
        "id": "v3_hD7YeyUcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In brief, **SortMergeJoin** is Spark’s default for large joins because it’s robust and memory-efficient compared to hash joins.\n",
        "\n"
      ],
      "metadata": {
        "id": "ALTb8-rn6X41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SET 7: ACTION-DRIVEN EXECUTION**"
      ],
      "metadata": {
        "id": "iLL8VjPQyZiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 7.1:** Create a long transformation pipeline without any action.\n",
        "\n",
        "**Task:** Explain what Spark has done so far."
      ],
      "metadata": {
        "id": "69hUbA45yZHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# long transformation pipeline\n",
        "long_pipeline_df = rides_df.filter(col(\"status\") == \"Completed\")\\\n",
        "                             .filter(col(\"distance_km\") > 5)\\\n",
        "                             .select(\"ride_id\", \"city\", \"distance_km\", \"duration_seconds\")\\\n",
        "                             .withColumn(\"duration_minutes\", col(\"duration_seconds\") / 60)\\\n",
        "                             .filter(col(\"duration_minutes\") < 10)"
      ],
      "metadata": {
        "id": "7tyj0oUn6yPu"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Explain what Spark has done so far.\n",
        "\n",
        "> Since no action has been called, Spark has not actually executed any computations yet. It has merely built a logical plan (or DAG) of these transformations in memory."
      ],
      "metadata": {
        "id": "iR3b0mUV7Iyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 7.2:** Trigger different actions (count, show, write) separately.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Observe whether Spark recomputes the DAG.\n",
        "2. Explain behavior."
      ],
      "metadata": {
        "id": "E3f3Vkj-fUrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# triggering count()\n",
        "row_count = long_pipeline_df.count()\n",
        "print(f\"Row count: {row_count}\")\n",
        "\n",
        "# Explanation: Spark executed the entire DAG up to this point to compute the count."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ffbtE1x7eVa",
        "outputId": "857e6762-5575-4ca7-92b5-e62bc273ac87"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row count: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# triggering show()\n",
        "long_pipeline_df.show()\n",
        "\n",
        "# Explanation: Spark re-executed the entire DAG to display the results."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7-0OubrAdYU",
        "outputId": "4b96e2e1-edb9-451b-9fe6-fdde168f5ad0"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-----------+----------------+------------------+\n",
            "|ride_id|     city|distance_km|duration_seconds|  duration_minutes|\n",
            "+-------+---------+-----------+----------------+------------------+\n",
            "|   R001|Hyderabad|       12.5|             240|               4.0|\n",
            "|   R002|    Delhi|        8.2|             180|               3.0|\n",
            "|   R004|Bangalore|        5.5|             120|               2.0|\n",
            "|   R005|Hyderabad|       20.0|             360|               6.0|\n",
            "|   R006|    Delhi|       25.0|             420|               7.0|\n",
            "|   R007|   Mumbai|        7.5|             150|               2.5|\n",
            "|   R008|Bangalore|       18.0|             330|               5.5|\n",
            "|   R010|Hyderabad|       10.0|             200|3.3333333333333335|\n",
            "+-------+---------+-----------+----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# triggering write()\n",
        "long_pipeline_df.write.mode(\"overwrite\").parquet(\"long_pipeline_df.parquet\")\n",
        "\n",
        "# Explanation: Spark re-executed the entire DAG again to write the data."
      ],
      "metadata": {
        "id": "OSa4ZiwDAjBH"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2:** Explain behavior.\n",
        "\n",
        "> Spark's operations are lazy. This means that when you define a series of transformations (like filter, select),\n",
        "Spark doesn't immediately compute the results. Instead, it builds a Directed Acyclic Graph (DAG) of operations.\n",
        "The actual computation only kicks off when an 'action' is called (like count(), show(), collect(), write()).\n",
        "\n",
        "> Crucially, for each action, Spark, will re-evaluate and re-execute the *entire* DAG from the source.\n",
        "\n",
        "> This behavior can lead to inefficiencies if you perform multiple actions on the same transformed DataFrame.\n",
        "To avoid recomputing the same transformations repeatedly, you would typically use caching (e.g., **.cache()** or **.persist()**)\n",
        "on intermediate DataFrames that are going to be used in multiple subsequent actions."
      ],
      "metadata": {
        "id": "BaldWwhfAl6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SET 8: THINKING QUESTIONS**"
      ],
      "metadata": {
        "id": "L-GHxuuCy2cS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why does broadcast remove shuffle from the DAG?\n",
        "\n",
        "> Broadcast removes shuffle from the DAG because:\n",
        "\n",
        "> * In a normal join (like SortMergeJoin), Spark shuffles both datasets so that rows with the same join key end up in the same partition.\n",
        "\n",
        "> * With BroadcastHashJoin, Spark broadcasts the smaller dataset to all executors. This means the big dataset doesn't need to move, and each executor can join locally.\n",
        "\n",
        "> Result: No need to repartition or shuffle the big dataset, because the join can happen locally using the broadcasted copy."
      ],
      "metadata": {
        "id": "JYzbSYZMy-2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Why does repartition always introduce shuffle?\n",
        "\n",
        "> **repartition()** changes the number of partitions and redistributes data evenly. To do this, Spark must move data across the cluster, which is a shuffle."
      ],
      "metadata": {
        "id": "nzebmQ-S3IoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why is coalesce cheaper than repartition?\n",
        "\n",
        "> **coalesce()** only reduces partitions by merging existing ones.\n",
        "It does not shuffle data, so it's a narrow transformation (fast).\n",
        "\n",
        "> **repartition()** shuffles everything to balance partitions, so it's expensive.\n",
        "\n"
      ],
      "metadata": {
        "id": "zXNUt4B_3Jiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why does Spark delay execution until an action?\n",
        "\n",
        "> Spark uses lazy evaluation:\n",
        "\n",
        "> * Transformations (map, filter, join) just build a plan.\n",
        "Spark waits for an action (collect, count, save) to know what result you need.\n",
        "\n",
        "> * This allows Spark to optimize the whole job before running it."
      ],
      "metadata": {
        "id": "9TOrwG6Y3L8E"
      }
    }
  ]
}